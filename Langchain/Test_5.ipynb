{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with document and chat history\n",
    "This script demonstrates how to create a chatbot that retrieves information from a web document using LangChain and FAISS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parrent_path = os.path.abspath(os.path.join(current_path, '..'))\n",
    "sys.path.append(parrent_path)\n",
    "\n",
    "from Update_Git import git_add, git_commit, git_push\n",
    "\n",
    "file_path = os.path.join(current_path, 'Test_5.ipynb')\n",
    "git_add(file_path)\n",
    "git_commit('Updated test 5')\n",
    "git_push('main')\n",
    "\n",
    "os.environ.pop(\"OPENAI_API_KEY\", None) # Because it loads a key from some place I dont know!\n",
    "env_path = os.path.join(current_path, \".env\")\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taha/Documents/Python_codes/Dev-playground/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3672: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  Hello! How can I assist you today?\n",
      "AI:  The document attached seems to be a list of selected references related to FreeSurfer, a software suite used for the analysis and visualization of structural and functional neuroimaging data. If you need more specific information or assistance regarding FreeSurfer or any related topic, feel free to ask!\n",
      "AI:  FreeSurfer is a brain imaging software package originally developed by Bruce Fischl, Anders Dale, Martin Sereno, and Doug Greve. It is now primarily maintained by the Laboratory for Computational Neuroimaging at the Athinoula A. Martinos Center for Biomedical Imaging. FreeSurfer includes a set of programs designed for analyzing magnetic resonance imaging (MRI) data of the brain. It is used for tasks such as segmentation, cortical surface reconstruction, volumetric analysis, and more. FreeSurfer is available for Mac OS and Linux operating systems, and while it can be downloaded and installed for free, a license key is required to run the software.\n",
      "AI:  It seems there might have been a typo or misunderstanding. There is no specific reference to \"OPETIA\" in the context provided. If you have a different question or need information on a specific topic, feel free to ask, and I'll be happy to help!\n"
     ]
    }
   ],
   "source": [
    "# LangChain Core\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser, JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# LangChain OpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# LangChain Chains\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# LangChain Community\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "# LangChain Utilities\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Function to load and split documents from a given URL\n",
    "def document_loader(url):\n",
    "    loader = WebBaseLoader(url)  # Load the document from the web\n",
    "    docs = loader.load()  # Extract the content\n",
    "    \n",
    "    # Split the document into smaller chunks for processing\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,  # Each chunk has a max size of 200 characters\n",
    "        chunk_overlap=50  # Overlapping of 50 characters between chunks for context continuity\n",
    "    )\n",
    "    split_docs = splitter.split_documents(docs)  # Split the documents\n",
    "\n",
    "    return split_docs  # Return the processed document chunks\n",
    "\n",
    "# Function to create a FAISS vector database\n",
    "def create_db(docs):\n",
    "    embedding = OpenAIEmbeddings()  # Generate embeddings using OpenAI\n",
    "    vector_store = FAISS.from_documents(docs, embedding=embedding)  # Store documents in FAISS index\n",
    "    return vector_store  # Return the FAISS vector store\n",
    "\n",
    "# Function to create a retrieval chain using the vector store\n",
    "def create_chain(vector_store):\n",
    "    model = ChatOpenAI(\n",
    "        model='gpt-3.5-turbo',  # Use GPT-3.5-turbo model\n",
    "        temperature=0.4  # Set temperature for response variability\n",
    "    )\n",
    "\n",
    "    # Define a prompt template for the chatbot\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the user's question based on the given context: {context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    parser = StrOutputParser()  # Convert output to a string format\n",
    "\n",
    "    # Create the document processing chain\n",
    "    chain = create_stuff_documents_chain(\n",
    "        llm=model,\n",
    "        prompt=prompt,\n",
    "        output_parser=parser\n",
    "    )\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwaargs = {\"k\": 3})  # Convert vector store into a retriever\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        retriever, \n",
    "        chain\n",
    "        )  # Create a retrieval-based chain\n",
    "\n",
    "    return retrieval_chain  # Return the retrieval chain\n",
    "\n",
    "\n",
    "def process_chat(chain, question, history):\n",
    "    # Query the retrieval chain with a sample question\n",
    "    response = chain.invoke({\n",
    "    \"input\": question,\n",
    "    \"chat_history\": history,\n",
    "    \"context\": docs\n",
    "})\n",
    "\n",
    "    # Print the answer from the model\n",
    "    return response[\"answer\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    docs = document_loader(\"https://en.wikipedia.org/wiki/FreeSurfer\")\n",
    "    vector_store = create_db(docs)\n",
    "    chain = create_chain(vector_store)\n",
    "\n",
    "    chat_history = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        response = process_chat(chain, user_input, chat_history)\n",
    "        chat_history.append(HumanMessage(content=user_input))\n",
    "        chat_history.append(AIMessage(content=response))\n",
    "        print(\"AI: \", response)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
