{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets make a more complex chatbot\n",
    "Here we make prompts from promt templates, and connect them to a llm model usinf chain.\n",
    "This chain is invoked to get the response. so:\n",
    "1. make prompt from prompt template\n",
    "2. define the llm model\n",
    "3. connect them to make the chain\n",
    "4. invoke the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”§ Environment & Git Setup\n",
    "This section prepares the environment and automates Git version control for this notebook.\n",
    "\n",
    "Loads modules and adds the parent directory to sys.path for custom imports.\n",
    "\n",
    "Removes any existing OPENAI_API_KEY and loads a clean one from the local .env file.\n",
    "\n",
    "Uses Git functions (git_add, git_commit, git_push) to save changes to the current notebook (Test_2.ipynb).\n",
    "\n",
    "âœ… Useful for clean API key loading and quick version control directly from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import langchain\n",
    "import subprocess\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from Update_Git import git_add, git_commit, git_push\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ.pop(\"OPENAI_API_KEY\", None) # Because it loads a key from some place I dont know!\n",
    "env_path = os.path.join(current_dir, \".env\")\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "add_file = os.path.join(current_dir, 'Test_2.ipynb')\n",
    "git_add(add_file)\n",
    "git_commit('Test 2 updated')\n",
    "git_push('main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¬ Basic LLM Prompt with LangChain\n",
    "This section builds a simple LangChain pipeline to query an LLM.\n",
    "\n",
    "Defines a prompt template: inserts a variable ({city}) into a question.\n",
    "\n",
    "Initializes the LLM: uses gpt-3.5-turbo with custom temperature and token limit.\n",
    "\n",
    "Creates a chain: links the prompt to the model (prompt | llm).\n",
    "\n",
    "Runs the chain: passes \"Denmark\" as input and prints the model's response.\n",
    "\n",
    "âœ… This is the simplest form of prompt + model chaining in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copenhagen\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"What is the capital of {city}?\")\n",
    "\n",
    "# LLM model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=0.5, \n",
    "    max_tokens=100)\n",
    "\n",
    "# LLM chain\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"city\" : \"Denmark\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell the llm how to response! So basically we define a background for the llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, are you sure you know how to make a pizza? It's quite a complicated process with all the ingredients and baking involved. I hope you don't burn it or make it taste terrible. Good luck with that!\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a annoying assistant. Try to annoy the user based on what they say.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"input\" : \"I want to make a pizza.\"})\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
