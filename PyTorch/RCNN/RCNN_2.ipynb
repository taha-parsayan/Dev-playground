{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI\n",
    "\n",
    "Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\n",
    "\n",
    ".\\packages\\Scripts\\Activate.ps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms, utils\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import cv2\n",
    "from torchvision.io import read_image\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from torchvision.datasets import VisionDataset\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python-codes\\packages\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Python-codes\\packages\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
       "=======================================================================================================================================\n",
       "FasterRCNN (FasterRCNN)                                 [16, 3, 224, 224]    [0, 4]               --                   Partial\n",
       "├─GeneralizedRCNNTransform (transform)                  [16, 3, 224, 224]    [16, 3, 800, 800]    --                   --\n",
       "├─BackboneWithFPN (backbone)                            [16, 3, 800, 800]    [16, 256, 13, 13]    --                   Partial\n",
       "│    └─IntermediateLayerGetter (body)                   [16, 3, 800, 800]    [16, 2048, 25, 25]   --                   Partial\n",
       "│    │    └─Conv2d (conv1)                              [16, 3, 800, 800]    [16, 64, 400, 400]   (9,408)              False\n",
       "│    │    └─FrozenBatchNorm2d (bn1)                     [16, 64, 400, 400]   [16, 64, 400, 400]   --                   --\n",
       "│    │    └─ReLU (relu)                                 [16, 64, 400, 400]   [16, 64, 400, 400]   --                   --\n",
       "│    │    └─MaxPool2d (maxpool)                         [16, 64, 400, 400]   [16, 64, 200, 200]   --                   --\n",
       "│    │    └─Sequential (layer1)                         [16, 64, 200, 200]   [16, 256, 200, 200]  (212,992)            False\n",
       "│    │    └─Sequential (layer2)                         [16, 256, 200, 200]  [16, 512, 100, 100]  1,212,416            True\n",
       "│    │    └─Sequential (layer3)                         [16, 512, 100, 100]  [16, 1024, 50, 50]   7,077,888            True\n",
       "│    │    └─Sequential (layer4)                         [16, 1024, 50, 50]   [16, 2048, 25, 25]   14,942,208           True\n",
       "│    └─FeaturePyramidNetwork (fpn)                      [16, 256, 200, 200]  [16, 256, 13, 13]    --                   True\n",
       "│    │    └─ModuleList (inner_blocks)                   --                   --                   (recursive)          True\n",
       "│    │    └─ModuleList (layer_blocks)                   --                   --                   (recursive)          True\n",
       "│    │    └─ModuleList (inner_blocks)                   --                   --                   (recursive)          True\n",
       "│    │    └─ModuleList (layer_blocks)                   --                   --                   (recursive)          True\n",
       "│    │    └─ModuleList (inner_blocks)                   --                   --                   (recursive)          True\n",
       "│    │    └─ModuleList (layer_blocks)                   --                   --                   (recursive)          True\n",
       "│    │    └─ModuleList (inner_blocks)                   --                   --                   (recursive)          True\n",
       "│    │    └─ModuleList (layer_blocks)                   --                   --                   (recursive)          True\n",
       "│    │    └─LastLevelMaxPool (extra_blocks)             [16, 256, 200, 200]  [16, 256, 200, 200]  --                   --\n",
       "├─RegionProposalNetwork (rpn)                           [16, 3, 800, 800]    [1000, 4]            --                   True\n",
       "│    └─RPNHead (head)                                   [16, 256, 200, 200]  [16, 3, 200, 200]    --                   True\n",
       "│    │    └─Sequential (conv)                           [16, 256, 200, 200]  [16, 256, 200, 200]  590,080              True\n",
       "│    │    └─Conv2d (cls_logits)                         [16, 256, 200, 200]  [16, 3, 200, 200]    771                  True\n",
       "│    │    └─Conv2d (bbox_pred)                          [16, 256, 200, 200]  [16, 12, 200, 200]   3,084                True\n",
       "│    │    └─Sequential (conv)                           [16, 256, 100, 100]  [16, 256, 100, 100]  (recursive)          True\n",
       "│    │    └─Conv2d (cls_logits)                         [16, 256, 100, 100]  [16, 3, 100, 100]    (recursive)          True\n",
       "│    │    └─Conv2d (bbox_pred)                          [16, 256, 100, 100]  [16, 12, 100, 100]   (recursive)          True\n",
       "│    │    └─Sequential (conv)                           [16, 256, 50, 50]    [16, 256, 50, 50]    (recursive)          True\n",
       "│    │    └─Conv2d (cls_logits)                         [16, 256, 50, 50]    [16, 3, 50, 50]      (recursive)          True\n",
       "│    │    └─Conv2d (bbox_pred)                          [16, 256, 50, 50]    [16, 12, 50, 50]     (recursive)          True\n",
       "│    │    └─Sequential (conv)                           [16, 256, 25, 25]    [16, 256, 25, 25]    (recursive)          True\n",
       "│    │    └─Conv2d (cls_logits)                         [16, 256, 25, 25]    [16, 3, 25, 25]      (recursive)          True\n",
       "│    │    └─Conv2d (bbox_pred)                          [16, 256, 25, 25]    [16, 12, 25, 25]     (recursive)          True\n",
       "│    │    └─Sequential (conv)                           [16, 256, 13, 13]    [16, 256, 13, 13]    (recursive)          True\n",
       "│    │    └─Conv2d (cls_logits)                         [16, 256, 13, 13]    [16, 3, 13, 13]      (recursive)          True\n",
       "│    │    └─Conv2d (bbox_pred)                          [16, 256, 13, 13]    [16, 12, 13, 13]     (recursive)          True\n",
       "│    └─AnchorGenerator (anchor_generator)               [16, 3, 800, 800]    [159882, 4]          --                   --\n",
       "├─RoIHeads (roi_heads)                                  [16, 256, 200, 200]  [0, 4]               --                   True\n",
       "│    └─MultiScaleRoIAlign (box_roi_pool)                [16, 256, 200, 200]  [16000, 256, 7, 7]   --                   --\n",
       "│    └─TwoMLPHead (box_head)                            [16000, 256, 7, 7]   [16000, 1024]        --                   True\n",
       "│    │    └─Linear (fc6)                                [16000, 12544]       [16000, 1024]        12,846,080           True\n",
       "│    │    └─Linear (fc7)                                [16000, 1024]        [16000, 1024]        1,049,600            True\n",
       "│    └─FastRCNNPredictor (box_predictor)                [16000, 1024]        [16000, 91]          --                   True\n",
       "│    │    └─Linear (cls_score)                          [16000, 1024]        [16000, 91]          93,275               True\n",
       "│    │    └─Linear (bbox_pred)                          [16000, 1024]        [16000, 364]         373,100              True\n",
       "=======================================================================================================================================\n",
       "Total params: 41,755,286\n",
       "Trainable params: 41,532,886\n",
       "Non-trainable params: 222,400\n",
       "Total mult-adds (T): 2.15\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 9.63\n",
       "Forward/backward pass size (MB): 23795.93\n",
       "Params size (MB): 167.02\n",
       "Estimated Total Size (MB): 23972.58\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "\n",
    "summary(model=model,\n",
    "        input_size=(16, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download and prepare data\n",
    "You'll need a labeled dataset for object detection in COCO-style format or VOC-style format. \n",
    "The Penn-Fudan dataset contains images of pedestrians and annotations for object detection (bounding boxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip to ./PennFudanPed\\PennFudanPed.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53.7M/53.7M [00:13<00:00, 3.94MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./PennFudanPed\\PennFudanPed.zip to ./PennFudanPed\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "url = \"https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\"\n",
    "root = \"./PennFudanPed\"\n",
    "download_and_extract_archive(url, root)\n",
    "\n",
    "# Define the dataset class\n",
    "class PennFudanDataset(VisionDataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        super().__init__(root)\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        # Convert the mask into a binary format (background=0, person=1)\n",
    "        mask = np.array(mask)\n",
    "        obj_ids = np.unique(mask)[1:]\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = PennFudanDataset(root=os.path.join(root, \"PennFudanPed\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packages",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
