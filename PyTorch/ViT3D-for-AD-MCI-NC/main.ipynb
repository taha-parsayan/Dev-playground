{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of AD vs MCI vs NC Using the ResNet Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import shutil\n",
    "import data_manager as DM\n",
    "# import torchvision.models as models\n",
    "import torchvision.models.video as models\n",
    "from torchvision.models.video import R3D_18_Weights, r3d_18\n",
    "import data_setup, engine\n",
    "from helper_functions import plot_loss_curves\n",
    "from data_setup import create_dataloaders\n",
    "import engine\n",
    "from monai.transforms import Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.utils.data import DataLoader\n",
    "from helper_functions import plot_loss_curves\n",
    "import torchio as tio\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import scipy.io as sio\n",
    "import monai\n",
    "import nibabel as nib\n",
    "from Update_Git import git_add, git_commit, git_push\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the parrent path to current path because data is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing command: ['git', 'commit', '-m', 'Update main.ipynb']\n",
      "Error message: \n",
      "Error executing command: ['git', 'push', 'origin', 'main']\n",
      "Error message: To https://github.com/taha-parsayan/Dev-playground\n",
      " ! [rejected]        main -> main (fetch first)\n",
      "error: failed to push some refs to 'https://github.com/taha-parsayan/Dev-playground'\n",
      "hint: Updates were rejected because the remote contains work that you do\n",
      "hint: not have locally. This is usually caused by another repository pushing\n",
      "hint: to the same ref. You may want to first integrate the remote changes\n",
      "hint: (e.g., 'git pull ...') before pushing again.\n",
      "hint: See the 'Note about fast-forwards' in 'git push --help' for details.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "parrent_path = os.path.abspath(os.path.join(current_path, '..'))\n",
    "sys.path.append(parrent_path)\n",
    "\n",
    "file_path1 = \"main.ipynb\"\n",
    "file_path2 = \"data_manager.py\"\n",
    "file_path3 = \"data_setup.py\"\n",
    "file_path4 = \"engine.py\"\n",
    "file_path6 = \"helper_functions.py\"\n",
    "file_path7 = \"predictions.py\"\n",
    "\n",
    "path = os.path.join(current_path, file_path1)\n",
    "git_add(path)\n",
    "git_commit(\"Update main.ipynb\")\n",
    "git_push(\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manage data:\n",
    "\n",
    "✔ Read subject IDs from each sheet in Subject list.xlsx.\n",
    "\n",
    "✔ Create Data/AD, Data/MCI, Data/NC folders.\n",
    "\n",
    "✔ Find std_T1.nii for each subject inside ADNI/{subject_id}/.\n",
    "\n",
    "✔ Copy & renames the file to Data/{category}/{subject_id}.nii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"AD\", \"MCI\", \"NC\"]\n",
    "excel_file = \"../Subject list.xlsx\"\n",
    "source_root = \"ADNI\"\n",
    "destination_root = \"Data\"\n",
    "if os.path.join(destination_root):\n",
    "   os.system('rm -r Data')\n",
    "source_dir = \"../\"\n",
    "DM.copy_data(excel_file,source_root,source_dir, destination_root,categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many subjects do we have in each group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"Data\"\n",
    "\n",
    "for c in categories:\n",
    "    path_train = os.path.join(data_root, 'train', c)\n",
    "    path_test = os.path.join(data_root, 'test', c)\n",
    "\n",
    "    num_train_files = len(os.listdir(path_train))\n",
    "    print(f\"{c} train: {num_train_files} files\")\n",
    "\n",
    "    num_test_files = len(os.listdir(path_test))\n",
    "    print(f\"{c} test: {num_test_files} files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader: Prepare the data for model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is the ViT default transformation?\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "transform = pretrained_vit_weights.transforms()\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "# pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "# transform = pretrained_vit_weights.transforms()\n",
    "# print(transform)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "print(f\"transform: {transform}\")\n",
    "\n",
    "# Augmentaed dataset to be added to the main dataset\n",
    "augmentation_transform = transforms.Compose([\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust colors\n",
    "    transforms.RandomHorizontalFlip(p=1),  # Random horizontal flip\n",
    "    transforms.RandomVerticalFlip(p=0.3),  # Random vertical flip\n",
    "    transforms.RandomRotation(degrees=15, interpolation=InterpolationMode.BICUBIC),  # Random rotation\n",
    "    transforms.Resize(size=256, interpolation=InterpolationMode.BILINEAR),  # Resize to 256\n",
    "    transforms.CenterCrop(size=224),  # Center crop to 224x224\n",
    "    #transforms.ToTensor(),  # Convert image to tensor\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "# Data loader\n",
    "train_data_path = os.path.join(data_root,\"train\")\n",
    "test_data_path = os.path.join(data_root,\"test\")\n",
    "\n",
    "train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_data_path, \n",
    "    test_dir=test_data_path,\n",
    "    transform=transform,\n",
    "    batch_size=batch_size,\n",
    "    # augmentation_transform=augmentation_transform\n",
    ")\n",
    "\n",
    "print(' ')\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "\n",
    "print(' ')\n",
    "print(\"Number of training data: \", len(train_dataloader_pretrained) * batch_size)\n",
    "print(\"Number of testing data: \", len(test_dataloader_pretrained) * batch_size)\n",
    "\n",
    "image_batch, label_batch = next(iter(train_dataloader_pretrained))\n",
    "print(image_batch.shape, label_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 9 images (first slice of each)\n",
    "num_images = 6\n",
    "image_batch_np = image_batch.cpu()  # Move to CPU\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 9))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):  \n",
    "    img = image_batch_np[i, 30]  # Take one slice (shape: 1, 224, 224)\n",
    "    img_np = img.numpy().transpose(1, 2, 0)  # Convert from (C, H, W) → (H, W, C)\n",
    "    ax.imshow(img_np)  # Show RGB image\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Label: {class_names[label_batch[i].item()]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained ViT model\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# Freeze pretrained weights\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Modify the conv_proj to accept 3-channel images (RGB slices)\n",
    "pretrained_vit.conv_proj = nn.Conv2d(\n",
    "    in_channels=1,  # 3-channel input (RGB slices)\n",
    "    out_channels=768,  # Output channels must match ViT embeddings\n",
    "    kernel_size=16,\n",
    "    stride=16\n",
    ").to(device)\n",
    "\n",
    "# Remove the classification head as we'll define our own\n",
    "pretrained_vit.heads = nn.Identity()\n",
    "\n",
    "# Define the ViT3D model with aggregation\n",
    "class ViT3D(nn.Module):\n",
    "    def __init__(self, vit_model, num_slices=79, num_classes=3, aggregation=\"mean\"):\n",
    "        super(ViT3D, self).__init__()\n",
    "        self.vit = vit_model\n",
    "        self.num_slices = num_slices\n",
    "        self.aggregation = aggregation  # \"mean\", \"max\" for feature aggregation\n",
    "        \n",
    "        # Fully connected classification head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(768, num_classes)  # Output dimension matches the number of classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_slices, C, H, W = x.shape  # Shape: (B, 79, 3, 224, 224)\n",
    "        x = x.view(batch_size * num_slices, C, H, W)  # Flatten slices: (B*79, 3, 224, 224)\n",
    "        \n",
    "        features = self.vit(x)  # Shape: (B*79, 768) after passing through ViT\n",
    "        \n",
    "        features = features.view(batch_size, num_slices, -1)  # Reshape to (B, 79, 768)\n",
    "\n",
    "        # Aggregate features across slices using specified method\n",
    "        if self.aggregation == \"mean\":\n",
    "            features = torch.mean(features, dim=1)  # Mean pooling across slices\n",
    "        elif self.aggregation == \"max\":\n",
    "            features, _ = torch.max(features, dim=1)  # Max pooling across slices\n",
    "        else:\n",
    "            raise ValueError(\"Invalid aggregation method. Choose 'mean' or 'max'.\")\n",
    "\n",
    "        return self.fc(features)  # Final classification step\n",
    "    \n",
    "# Initialize the ViT3D model\n",
    "vit3d_model = ViT3D(pretrained_vit, num_slices=79, num_classes=3, aggregation=\"mean\").to(device)\n",
    "\n",
    "# Print the model summary\n",
    "summary(model=vit3d_model,\n",
    "        input_size=(8, 79, 1, 224, 224),  # Input shape: (batch_size, slices, channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the device\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Working on device: {device}\")\n",
    "\n",
    "# Create optimizer and loss function\n",
    "optimizer = torch.optim.Adam(vit3d_model.parameters(), lr=1e-64, weight_decay=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the classifier head of the pretrained ViT feature extractor model\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "pretrained_RN_18_results = engine.train(\n",
    "    model=vit3d_model,\n",
    "    train_dataloader=train_dataloader_pretrained,\n",
    "    test_dataloader=test_dataloader_pretrained,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=5,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(vit3d_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packages",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
